{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68479,"databundleVersionId":7609535,"sourceType":"competition"},{"sourceId":7009925,"sourceType":"datasetVersion","datasetId":4030196}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shiwayz/obesity-classification-98-11-orig-91-47-comp?scriptVersionId=162397511\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"'''\n\nhttps://www.kaggle.com/code/oscarm524/ps-s3-ep23-eda-modeling-submission/notebook\n\nTOP MODELS\n-- XGBoost\n-- LGBM\n-- CatBoost\n-- Hist GradBoost\n\n'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport logging\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import randint, uniform\n\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier,  HistGradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom lightgbm import LGBMClassifier\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:15:09.188992Z","iopub.execute_input":"2024-02-07T15:15:09.189827Z","iopub.status.idle":"2024-02-07T15:15:17.171846Z","shell.execute_reply.started":"2024-02-07T15:15:09.189788Z","shell.execute_reply":"2024-02-07T15:15:17.170211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp_df = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\noriginal_df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\n\nrandom_state = 42","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:15:17.174915Z","iopub.execute_input":"2024-02-07T15:15:17.17544Z","iopub.status.idle":"2024-02-07T15:15:17.482894Z","shell.execute_reply.started":"2024-02-07T15:15:17.175394Z","shell.execute_reply":"2024-02-07T15:15:17.481489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:red\"> Optuna Ensemble Original Data</span>\n","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> Without CV, XGB+LGBM [97.87%]</span>\n","metadata":{}},{"cell_type":"code","source":"original_df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\n\ndf = original_df.copy()\ncategorical_features = df.select_dtypes(include=['object']).columns\nnon_categorical_features = df.select_dtypes(exclude=['object'])\n\ndf['CALC'] = np.where(df['CALC'] == 'Always', 'Frequently', df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ndf[categorical_features] = ordinal_encoder.fit_transform(df[categorical_features])\n\nscaler = StandardScaler()\ndf[non_categorical_features.columns] = scaler.fit_transform(df[non_categorical_features.columns])\n\ny = df['NObeyesdad']\nX = df.drop(columns=['NObeyesdad'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\naccuracies = []\n\ndef objective(trial):\n    \n    # LightGBM parameters\n    lgb_params = {\n        'objective': 'multiclass',\n        'num_class': 7,\n        'metric': 'multi_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'random_state' : random_state,\n        'lambda_l1': trial.suggest_float('lgb_lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lgb_lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('lgb_num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('lgb_feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('lgb_bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('lgb_bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 100),\n        'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3)\n    }\n\n    # XGBoost parameters\n    xgb_params = {\n        'objective': 'multi:softprob',\n        'num_class': 7,\n        'verbosity': 0,\n        'random_state' : random_state,\n        'lambda': trial.suggest_float('xgb_lambda', 1e-8, 10.0, log=True),\n        'alpha': trial.suggest_float('xgb_alpha', 1e-8, 10.0, log=True),\n        'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n        'eta': trial.suggest_float('xgb_eta', 0.01, 0.3),\n        'subsample': trial.suggest_float('xgb_subsample', 0.4, 1.0),\n        'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.4, 1.0),\n    }\n\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n\n    # Predictions\n    lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n    xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n\n    ensemble_pred = np.mean([lgb_pred, xgb_pred], axis=0)\n\n    final_pred = np.argmax(ensemble_pred, axis=1)\n\n    accuracy = accuracy_score(y_test, final_pred)\n    accuracies.append(accuracy)\n    \n    return accuracy \n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\nprint(\"Best trial:\", study.best_trial.params)\n\nplt.figure(figsize=(12, 6))\nplt.plot(accuracies, label='Accuracy')\nplt.title('Accuracy over Trials')\nplt.xlabel('Trial')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T09:30:04.041936Z","iopub.execute_input":"2024-02-07T09:30:04.042468Z","iopub.status.idle":"2024-02-07T09:34:50.498396Z","shell.execute_reply.started":"2024-02-07T09:30:04.042408Z","shell.execute_reply":"2024-02-07T09:34:50.497258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T09:35:46.393055Z","iopub.execute_input":"2024-02-07T09:35:46.393802Z","iopub.status.idle":"2024-02-07T09:35:46.40093Z","shell.execute_reply.started":"2024-02-07T09:35:46.393766Z","shell.execute_reply":"2024-02-07T09:35:46.399809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nbest_trial_params = {\n    'lgb_lambda_l1': 0.00022450967926345529,\n    'lgb_lambda_l2': 3.0346284577653767e-07,\n    'lgb_num_leaves': 147,\n    'lgb_feature_fraction': 0.7138759815745731,\n    'lgb_bagging_fraction': 0.9295853923373891,\n    'lgb_bagging_freq': 2,\n    'lgb_min_child_samples': 48,\n    'lgb_learning_rate': 0.18936631482820396,\n    'xgb_lambda': 5.363552546585817e-06,\n    'xgb_alpha': 0.002670337201689084,\n    'xgb_max_depth': 8,\n    'xgb_eta': 0.09938218925297368,\n    'xgb_subsample': 0.6931943353971597,\n    'xgb_colsample_bytree': 0.7136240974017022\n}\n\n# Converted parameters\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state' : 42,\n    'lambda_l1': best_trial_params['lgb_lambda_l1'],\n    'lambda_l2': best_trial_params['lgb_lambda_l2'],\n    'num_leaves': best_trial_params['lgb_num_leaves'],\n    'feature_fraction': best_trial_params['lgb_feature_fraction'],\n    'bagging_fraction': best_trial_params['lgb_bagging_fraction'],\n    'bagging_freq': best_trial_params['lgb_bagging_freq'],\n    'min_child_samples': best_trial_params['lgb_min_child_samples'],\n    'learning_rate': best_trial_params['lgb_learning_rate']\n}\n\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state' : 42,\n    'lambda': best_trial_params['xgb_lambda'],\n    'alpha': best_trial_params['xgb_alpha'],\n    'max_depth': best_trial_params['xgb_max_depth'],\n    'eta': best_trial_params['xgb_eta'],\n    'subsample': best_trial_params['xgb_subsample'],\n    'colsample_bytree': best_trial_params['xgb_colsample_bytree']\n}\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> With CV, XGB+LGBM [97.92%]</span>\n","metadata":{}},{"cell_type":"code","source":"original_df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\n\ndf = original_df.copy()\ncategorical_features = df.select_dtypes(include=['object']).columns\nnon_categorical_features = df.select_dtypes(exclude=['object'])\n\ndf['CALC'] = np.where(df['CALC'] == 'Always', 'Frequently', df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ndf[categorical_features] = ordinal_encoder.fit_transform(df[categorical_features])\n\nscaler = StandardScaler()\ndf[non_categorical_features.columns] = scaler.fit_transform(df[non_categorical_features.columns])\n\ny = df['NObeyesdad']\nX = df.drop(columns=['NObeyesdad'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\naccuracies = []\n\ndef objective(trial):\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    accuracies = []\n    \n    for train_index, test_index in kf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # LightGBM parameters\n        lgb_params = {\n            'objective': 'multiclass',\n            'num_class': 7,\n            'metric': 'multi_logloss',\n            'verbosity': -1,\n            'boosting_type': 'gbdt',\n            'random_state' : random_state,\n            'lambda_l1': trial.suggest_float('lgb_lambda_l1', 1e-8, 10.0, log=True),\n            'lambda_l2': trial.suggest_float('lgb_lambda_l2', 1e-8, 10.0, log=True),\n            'num_leaves': trial.suggest_int('lgb_num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_float('lgb_feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_float('lgb_bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('lgb_bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 100),\n            'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3)\n        }\n        \n        # XGBoost parameters\n        xgb_params = {\n            'objective': 'multi:softprob',\n            'num_class': 7,\n            'verbosity': 0,\n            'random_state' : random_state,\n            'lambda': trial.suggest_float('xgb_lambda', 1e-8, 10.0, log=True),\n            'alpha': trial.suggest_float('xgb_alpha', 1e-8, 10.0, log=True),\n            'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n            'eta': trial.suggest_float('xgb_eta', 0.01, 0.3),\n            'subsample': trial.suggest_float('xgb_subsample', 0.4, 1.0),\n            'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.4, 1.0),\n        }\n\n        # Training LightGBM\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n        # Training XGBoost\n        xgb_train = xgb.DMatrix(X_train, label=y_train)\n        xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n\n        lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n        xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n\n        ensemble_pred = np.mean([lgb_pred, xgb_pred], axis=0)\n        final_pred = np.argmax(ensemble_pred, axis=1)\n\n        accuracy = accuracy_score(y_test, final_pred)\n        accuracies.append(accuracy)\n        \n    mean_accuracies.append(np.mean(accuracies))\n    return np.mean(accuracies)\n            \noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\nprint(\"Best trial:\", study.best_trial.params)\nprint(\"\\n Best score:\", study.best_trial.value, \"\\n\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(mean_accuracies, label='Accuracy')\nplt.title('Accuracy over Trials')\nplt.xlabel('Trial')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nbest_trial_params = {\n    'lgb_lambda_l1': 1.7468312934870682e-08,\n    'lgb_lambda_l2': 4.514597403500374e-07,\n    'lgb_num_leaves': 108,\n    'lgb_feature_fraction': 0.8355879678246828,\n    'lgb_bagging_fraction': 0.9429435908403985,\n    'lgb_bagging_freq': 3,\n    'lgb_min_child_samples': 53,\n    'lgb_learning_rate': 0.13936934533091533,\n    'xgb_lambda': 1.549322630830155e-07,\n    'xgb_alpha': 0.002155193159445727,\n    'xgb_max_depth': 7,\n    'xgb_eta': 0.2604963851766582,\n    'xgb_subsample': 0.8935684621215217,\n    'xgb_colsample_bytree': 0.868808428718869\n}\n\n# Converted parameters\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state' : 42,\n    'lambda_l1': best_trial_params['lgb_lambda_l1'],\n    'lambda_l2': best_trial_params['lgb_lambda_l2'],\n    'num_leaves': best_trial_params['lgb_num_leaves'],\n    'feature_fraction': best_trial_params['lgb_feature_fraction'],\n    'bagging_fraction': best_trial_params['lgb_bagging_fraction'],\n    'bagging_freq': best_trial_params['lgb_bagging_freq'],\n    'min_child_samples': best_trial_params['lgb_min_child_samples'],\n    'learning_rate': best_trial_params['lgb_learning_rate']\n}\n\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state' : 42,\n    'lambda': best_trial_params['xgb_lambda'],\n    'alpha': best_trial_params['xgb_alpha'],\n    'max_depth': best_trial_params['xgb_max_depth'],\n    'eta': best_trial_params['xgb_eta'],\n    'subsample': best_trial_params['xgb_subsample'],\n    'colsample_bytree': best_trial_params['xgb_colsample_bytree']\n}\n\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> With CV, XGB+LGBM Best Parameters [97.92%]</span>\n","metadata":{}},{"cell_type":"code","source":"original_df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\n\ndf = original_df.copy()\ncategorical_features = df.select_dtypes(include=['object']).columns\nnon_categorical_features = df.select_dtypes(exclude=['object'])\n\ndf['CALC'] = np.where(df['CALC'] == 'Always', 'Frequently', df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ndf[categorical_features] = ordinal_encoder.fit_transform(df[categorical_features])\n\nscaler = StandardScaler()\ndf[non_categorical_features.columns] = scaler.fit_transform(df[non_categorical_features.columns])\n\ny = df['NObeyesdad']\nX = df.drop(columns=['NObeyesdad'])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\n\nbest_params = {\n    'lgb_lambda_l1': 1.7468312934870682e-08,\n    'lgb_lambda_l2': 4.514597403500374e-07,\n    'lgb_num_leaves': 108,\n    'lgb_feature_fraction': 0.8355879678246828,\n    'lgb_bagging_fraction': 0.9429435908403985,\n    'lgb_bagging_freq': 3,\n    'lgb_min_child_samples': 53,\n    'lgb_learning_rate': 0.13936934533091533,\n    'xgb_lambda': 1.549322630830155e-07,\n    'xgb_alpha': 0.002155193159445727,\n    'xgb_max_depth': 7,\n    'xgb_eta': 0.2604963851766582,\n    'xgb_subsample': 0.8935684621215217,\n    'xgb_colsample_bytree': 0.868808428718869\n}\n\n# LightGBM parameters\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state': 42,\n    'lambda_l1': best_params['lgb_lambda_l1'],\n    'lambda_l2': best_params['lgb_lambda_l2'],\n    'num_leaves': best_params['lgb_num_leaves'],\n    'feature_fraction': best_params['lgb_feature_fraction'],\n    'bagging_fraction': best_params['lgb_bagging_fraction'],\n    'bagging_freq': best_params['lgb_bagging_freq'],\n    'min_child_samples': best_params['lgb_min_child_samples'],\n    'learning_rate': best_params['lgb_learning_rate']\n}\n\n# XGBoost parameters\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state': 42,\n    'lambda': best_params['xgb_lambda'],\n    'alpha': best_params['xgb_alpha'],\n    'max_depth': best_params['xgb_max_depth'],\n    'eta': best_params['xgb_eta'],\n    'subsample': best_params['xgb_subsample'],\n    'colsample_bytree': best_params['xgb_colsample_bytree']\n}\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ntest_accuracies = []\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Training LightGBM\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n    # Training XGBoost\n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n\n    lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n    xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n\n    ensemble_pred = np.mean([lgb_pred, xgb_pred], axis=0)\n    final_pred = np.argmax(ensemble_pred, axis=1)\n\n    test_accuracy = accuracy_score(y_test, final_pred)\n    test_accuracies.append(test_accuracy)\n\nprint(\"Average test set accuracy:\", np.mean(test_accuracies))","metadata":{"execution":{"iopub.status.busy":"2024-02-07T10:10:39.956123Z","iopub.execute_input":"2024-02-07T10:10:39.95656Z","iopub.status.idle":"2024-02-07T10:10:55.571921Z","shell.execute_reply.started":"2024-02-07T10:10:39.956529Z","shell.execute_reply":"2024-02-07T10:10:55.571172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> With CV, XGB+LGBM+CB+HGB [98.11%]</span>\n","metadata":{}},{"cell_type":"code","source":"original_df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\n\ndf = original_df.copy()\ncategorical_features = df.select_dtypes(include=['object']).columns\nnon_categorical_features = df.select_dtypes(exclude=['object'])\n\ndf['CALC'] = np.where(df['CALC'] == 'Always', 'Frequently', df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ndf[categorical_features] = ordinal_encoder.fit_transform(df[categorical_features])\n\nscaler = StandardScaler()\ndf[non_categorical_features.columns] = scaler.fit_transform(df[non_categorical_features.columns])\n\ny = df['NObeyesdad']\nX = df.drop(columns=['NObeyesdad'])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\nmean_accuracies = []\n\ndef objective(trial):\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    accuracies = []\n    \n    for train_index, test_index in kf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # LightGBM parameters\n        lgb_params = {\n            'objective': 'multiclass',\n            'num_class': 7,\n            'metric': 'multi_logloss',\n            'verbosity': -1,\n            'boosting_type': 'gbdt',\n            'random_state' : 42,\n            'lambda_l1': trial.suggest_float('lgb_lambda_l1', 1e-8, 10.0, log=True),\n            'lambda_l2': trial.suggest_float('lgb_lambda_l2', 1e-8, 10.0, log=True),\n            'num_leaves': trial.suggest_int('lgb_num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_float('lgb_feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_float('lgb_bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('lgb_bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 100),\n            'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3)\n        }\n\n        # XGBoost parameters\n        xgb_params = {\n            'objective': 'multi:softprob',\n            'num_class': 7,\n            'verbosity': 0,\n            'random_state' : 42,\n            'lambda': trial.suggest_float('xgb_lambda', 1e-8, 10.0, log=True),\n            'alpha': trial.suggest_float('xgb_alpha', 1e-8, 10.0, log=True),\n            'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n            'eta': trial.suggest_float('xgb_eta', 0.01, 0.3),\n            'subsample': trial.suggest_float('xgb_subsample', 0.4, 1.0),\n            'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.4, 1.0),\n        }\n\n        # CatBoost parameters\n        cb_params = {\n            'objective': 'MultiClass',\n            'verbose': 0,\n            'random_seed': 42,\n            'iterations' : trial.suggest_int('iterations', 50, 500),\n            'l2_leaf_reg': trial.suggest_float('cb_l2_leaf_reg', 1e-8, 10.0, log=True),\n            'depth': trial.suggest_int('cb_depth', 3, 10),\n            'learning_rate': trial.suggest_float('cb_learning_rate', 0.01, 0.3),\n            'colsample_bylevel': trial.suggest_float('cb_colsample_bylevel', 0.4, 1.0),\n        }\n\n        # HistGradientBoosting parameters\n        hgb_params = {\n            'max_iter': 100,\n            'random_state' : random_state,\n            'learning_rate': trial.suggest_float('hgb_learning_rate', 0.01, 0.3),\n            'max_depth': trial.suggest_int('hgb_max_depth', 3, 10),\n            'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 5, 100),\n            'max_bins': trial.suggest_int('hgb_max_bins', 50, 255),\n        }\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n        xgb_train = xgb.DMatrix(X_train, label=y_train)\n        xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n\n        cb_model = cb.CatBoostClassifier(**cb_params)\n        cb_model.fit(X_train, y_train)\n\n        hgb_model = HistGradientBoostingClassifier(**hgb_params)\n        hgb_model.fit(X_train, y_train)\n\n        # Predictions\n        lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n        xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n        cb_pred = cb_model.predict_proba(X_test)\n        hgb_pred = hgb_model.predict_proba(X_test)\n\n        ensemble_pred = np.mean([lgb_pred, xgb_pred, cb_pred, hgb_pred], axis=0)\n\n        final_pred = np.argmax(ensemble_pred, axis=1)\n\n        accuracy = accuracy_score(y_test, final_pred)\n        accuracies.append(accuracy)\n        \n    mean_accuracies.append(np.mean(accuracies))\n    return np.mean(accuracies)\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\nprint(\"Best trial:\", study.best_trial.params)\nprint(\"\\n Best score:\", study.best_trial.value, \"\\n\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(mean_accuracies, label='Accuracy')\nplt.title('Accuracy over Trials')\nplt.xlabel('Trial')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T10:39:34.795535Z","iopub.execute_input":"2024-02-07T10:39:34.795963Z","iopub.status.idle":"2024-02-07T12:04:23.263235Z","shell.execute_reply.started":"2024-02-07T10:39:34.795932Z","shell.execute_reply":"2024-02-07T12:04:23.262284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nbest_trial_params = {\n    'lgb_lambda_l1': 6.428285788433516e-05,\n    'lgb_lambda_l2': 5.023813910951442e-06,\n    'lgb_num_leaves': 95,\n    'lgb_feature_fraction': 0.9090165394440615,\n    'lgb_bagging_fraction': 0.941177308780672,\n    'lgb_bagging_freq': 1,\n    'lgb_min_child_samples': 49,\n    'lgb_learning_rate': 0.17493106395036273,\n    'xgb_lambda': 0.0002340403415754626,\n    'xgb_alpha': 0.03382506356447083,\n    'xgb_max_depth': 10,\n    'xgb_eta': 0.056646106213682795,\n    'xgb_subsample': 0.6136751692955198,\n    'xgb_colsample_bytree': 0.5094177734897046,\n    'iterations': 313,\n    'cb_l2_leaf_reg': 2.1202520834506392e-07,\n    'cb_depth': 8,\n    'cb_learning_rate': 0.06816277412483977,\n    'cb_colsample_bylevel': 0.919245284773473,\n    'hgb_learning_rate': 0.2156436115943959,\n    'hgb_max_depth': 4,\n    'hgb_min_samples_leaf': 88,\n    'hgb_max_bins': 65\n}\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\n\ndf = original_df.copy()\ncategorical_features = df.select_dtypes(include=['object']).columns\nnon_categorical_features = df.select_dtypes(exclude=['object'])\n\ndf['CALC'] = np.where(df['CALC'] == 'Always', 'Frequently', df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ndf[categorical_features] = ordinal_encoder.fit_transform(df[categorical_features])\n\nscaler = StandardScaler()\ndf[non_categorical_features.columns] = scaler.fit_transform(df[non_categorical_features.columns])\n\ny = df['NObeyesdad']\nX = df.drop(columns=['NObeyesdad'])\n\nbest_trial_params = {\n    'lgb_lambda_l1': 6.428285788433516e-05,\n    'lgb_lambda_l2': 5.023813910951442e-06,\n    'lgb_num_leaves': 95,\n    'lgb_feature_fraction': 0.9090165394440615,\n    'lgb_bagging_fraction': 0.941177308780672,\n    'lgb_bagging_freq': 1,\n    'lgb_min_child_samples': 49,\n    'lgb_learning_rate': 0.17493106395036273,\n    'xgb_lambda': 0.0002340403415754626,\n    'xgb_alpha': 0.03382506356447083,\n    'xgb_max_depth': 10,\n    'xgb_eta': 0.056646106213682795,\n    'xgb_subsample': 0.6136751692955198,\n    'xgb_colsample_bytree': 0.5094177734897046,\n    'iterations': 313,\n    'cb_l2_leaf_reg': 2.1202520834506392e-07,\n    'cb_depth': 8,\n    'cb_learning_rate': 0.06816277412483977,\n    'cb_colsample_bylevel': 0.919245284773473,\n    'hgb_learning_rate': 0.2156436115943959,\n    'hgb_max_depth': 4,\n    'hgb_min_samples_leaf': 88,\n    'hgb_max_bins': 65\n}\n\n\n# LightGBM parameters\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state': random_state,\n    'lambda_l1': best_trial_params['lgb_lambda_l1'],\n    'lambda_l2': best_trial_params['lgb_lambda_l2'],\n    'num_leaves': best_trial_params['lgb_num_leaves'],\n    'feature_fraction': best_trial_params['lgb_feature_fraction'],\n    'bagging_fraction': best_trial_params['lgb_bagging_fraction'],\n    'bagging_freq': best_trial_params['lgb_bagging_freq'],\n    'min_child_samples': best_trial_params['lgb_min_child_samples'],\n    'learning_rate': best_trial_params['lgb_learning_rate']\n}\n\n# XGBoost parameters\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state': 42,\n    'lambda': best_trial_params['xgb_lambda'],\n    'alpha': best_trial_params['xgb_alpha'],\n    'max_depth': best_trial_params['xgb_max_depth'],\n    'eta': best_trial_params['xgb_eta'],\n    'subsample': best_trial_params['xgb_subsample'],\n    'colsample_bytree': best_trial_params['xgb_colsample_bytree']\n}\n\n# CatBoost parameters\ncb_params = {\n    'objective': 'MultiClass',\n    'verbose': 0,\n    'random_seed': random_state,\n    'iterations': best_trial_params['iterations'],\n    'learning_rate': best_trial_params['cb_learning_rate'],\n    'depth': best_trial_params['cb_depth'],\n    'l2_leaf_reg': best_trial_params['cb_l2_leaf_reg'],\n    'colsample_bylevel': best_trial_params['cb_colsample_bylevel']\n}\n\n# HistBoost parameters\nhgb_params = {\n    'max_iter': 100,\n    'random_state' : random_state,\n    'learning_rate': best_trial_params['hgb_learning_rate'],\n    'max_depth': best_trial_params['hgb_max_depth'],\n    'min_samples_leaf': best_trial_params['hgb_min_samples_leaf'],\n    'max_bins': best_trial_params['hgb_max_bins']\n}\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ntest_accuracies = []\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Training LightGBM\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n    # Training XGBoost\n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n    \n    # Training CatBoost\n    cb_model = cb.CatBoostClassifier(**cb_params)\n    cb_model.fit(X_train, y_train)\n    \n    # Training HistGradBoost\n    hgb_model = HistGradientBoostingClassifier(**hgb_params)\n    hgb_model.fit(X_train, y_train)\n\n    lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n    xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n    cb_pred = cb_model.predict_proba(X_test)\n    hgb_pred = hgb_model.predict_proba(X_test)\n\n    ensemble_pred = np.mean([lgb_pred, xgb_pred, cb_pred, hgb_pred], axis=0)\n    final_pred = np.argmax(ensemble_pred, axis=1)\n\n    test_accuracy = accuracy_score(y_test, final_pred)\n    test_accuracies.append(test_accuracy)\n\nprint(\"Average test set accuracy:\", np.mean(test_accuracies))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T12:13:36.495893Z","iopub.execute_input":"2024-02-07T12:13:36.496313Z","iopub.status.idle":"2024-02-07T12:14:34.478095Z","shell.execute_reply.started":"2024-02-07T12:13:36.496275Z","shell.execute_reply":"2024-02-07T12:14:34.476642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:red\"> Optuna Ensemble Original+Comp Data</span>","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> Optuna Search XGB + LGBM + CB + HGB [91.77%]</span>\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\ncomp_df = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ncomp_df = comp_df.drop(columns=['id'])\n\ncombined_df = pd.concat([df, comp_df], axis=0)\ncombined_df = combined_df.drop_duplicates()\ncombined_df.reset_index(drop=True, inplace=True)\n\ncategorical_features = combined_df.select_dtypes(include=['object']).columns\ncategorical_features = [col for col in categorical_features if col != 'NObeyesdad']\nnon_categorical_features = combined_df.select_dtypes(exclude=['object'])\ncombined_df['CALC'] = np.where(combined_df['CALC'] == 'Always', 'Frequently', combined_df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ncombined_df[categorical_features] = ordinal_encoder.fit_transform(combined_df[categorical_features])\n\nscaler = StandardScaler()\ncombined_df[non_categorical_features.columns] = scaler.fit_transform(combined_df[non_categorical_features.columns])\n\nX = combined_df.drop(columns=['NObeyesdad'])\ny = combined_df['NObeyesdad']\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\nmean_accuracies = []\n\ndef objective(trial):\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    accuracies = []\n    \n    for train_index, test_index in kf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # LightGBM parameters\n        lgb_params = {\n            'objective': 'multiclass',\n            'num_class': 7,\n            'metric': 'multi_logloss',\n            'verbosity': -1,\n            'boosting_type': 'gbdt',\n            'random_state' : 42,\n            'lambda_l1': trial.suggest_float('lgb_lambda_l1', 1e-8, 10.0, log=True),\n            'lambda_l2': trial.suggest_float('lgb_lambda_l2', 1e-8, 10.0, log=True),\n            'num_leaves': trial.suggest_int('lgb_num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_float('lgb_feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_float('lgb_bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('lgb_bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('lgb_min_child_samples', 5, 100),\n            'learning_rate': trial.suggest_float('lgb_learning_rate', 0.01, 0.3)\n        }\n\n        # XGBoost parameters\n        xgb_params = {\n            'objective': 'multi:softprob',\n            'num_class': 7,\n            'verbosity': 0,\n            'random_state' : 42,\n            'lambda': trial.suggest_float('xgb_lambda', 1e-8, 10.0, log=True),\n            'alpha': trial.suggest_float('xgb_alpha', 1e-8, 10.0, log=True),\n            'max_depth': trial.suggest_int('xgb_max_depth', 3, 10),\n            'eta': trial.suggest_float('xgb_eta', 0.01, 0.3),\n            'subsample': trial.suggest_float('xgb_subsample', 0.4, 1.0),\n            'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.4, 1.0),\n        }\n\n        # CatBoost parameters\n        cb_params = {\n            'objective': 'MultiClass',\n            'verbose': 0,\n            'random_seed': 42,\n            'iterations' : trial.suggest_int('iterations', 50, 500),\n            'l2_leaf_reg': trial.suggest_float('cb_l2_leaf_reg', 1e-8, 10.0, log=True),\n            'depth': trial.suggest_int('cb_depth', 3, 10),\n            'learning_rate': trial.suggest_float('cb_learning_rate', 0.01, 0.3),\n            'colsample_bylevel': trial.suggest_float('cb_colsample_bylevel', 0.4, 1.0),\n        }\n\n        # HistGradientBoosting parameters\n        hgb_params = {\n            'max_iter': 100,\n            'random_state' : random_state,\n            'learning_rate': trial.suggest_float('hgb_learning_rate', 0.01, 0.3),\n            'max_depth': trial.suggest_int('hgb_max_depth', 3, 10),\n            'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 5, 100),\n            'max_bins': trial.suggest_int('hgb_max_bins', 50, 255),\n        }\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n        xgb_train = xgb.DMatrix(X_train, label=y_train)\n        xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n\n        cb_model = cb.CatBoostClassifier(**cb_params)\n        cb_model.fit(X_train, y_train)\n\n        hgb_model = HistGradientBoostingClassifier(**hgb_params)\n        hgb_model.fit(X_train, y_train)\n\n        # Predictions\n        lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n        xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n        cb_pred = cb_model.predict_proba(X_test)\n        hgb_pred = hgb_model.predict_proba(X_test)\n\n        ensemble_pred = np.mean([lgb_pred, xgb_pred, cb_pred, hgb_pred], axis=0)\n\n        final_pred = np.argmax(ensemble_pred, axis=1)\n\n        accuracy = accuracy_score(y_test, final_pred)\n        accuracies.append(accuracy)\n        \n    mean_accuracies.append(np.mean(accuracies))\n    return np.mean(accuracies)\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100) \n\nprint(\"Best trial:\", study.best_trial.params)\nprint(\"\\n Best score:\", study.best_trial.value, \"\\n\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(mean_accuracies, label='Accuracy')\nplt.title('Accuracy over Trials')\nplt.xlabel('Trial')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T21:15:36.325868Z","iopub.execute_input":"2024-02-06T21:15:36.326315Z","iopub.status.idle":"2024-02-06T21:54:08.492822Z","shell.execute_reply.started":"2024-02-06T21:15:36.326285Z","shell.execute_reply":"2024-02-06T21:54:08.491405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> Testing Original Params on Original+Comp, [91.47%]  </span>\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\ncomp_df = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ncomp_df = comp_df.drop(columns=['id'])\n\ncombined_df = pd.concat([df, comp_df], axis=0)\ncombined_df = combined_df.drop_duplicates()\ncombined_df.reset_index(drop=True, inplace=True)\n\ncategorical_features = combined_df.select_dtypes(include=['object']).columns\ncategorical_features = [col for col in categorical_features if col != 'NObeyesdad']\nnon_categorical_features = combined_df.select_dtypes(exclude=['object'])\ncombined_df['CALC'] = np.where(combined_df['CALC'] == 'Always', 'Frequently', combined_df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ncombined_df[categorical_features] = ordinal_encoder.fit_transform(combined_df[categorical_features])\n\nscaler = StandardScaler()\ncombined_df[non_categorical_features.columns] = scaler.fit_transform(combined_df[non_categorical_features.columns])\n\nX = combined_df.drop(columns=['NObeyesdad'])\ny = combined_df['NObeyesdad']\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n\nbest_trial_params = {\n    'lgb_lambda_l1': 6.428285788433516e-05,\n    'lgb_lambda_l2': 5.023813910951442e-06,\n    'lgb_num_leaves': 95,\n    'lgb_feature_fraction': 0.9090165394440615,\n    'lgb_bagging_fraction': 0.941177308780672,\n    'lgb_bagging_freq': 1,\n    'lgb_min_child_samples': 49,\n    'lgb_learning_rate': 0.17493106395036273,\n    'xgb_lambda': 0.0002340403415754626,\n    'xgb_alpha': 0.03382506356447083,\n    'xgb_max_depth': 10,\n    'xgb_eta': 0.056646106213682795,\n    'xgb_subsample': 0.6136751692955198,\n    'xgb_colsample_bytree': 0.5094177734897046,\n    'iterations': 313,\n    'cb_l2_leaf_reg': 2.1202520834506392e-07,\n    'cb_depth': 8,\n    'cb_learning_rate': 0.06816277412483977,\n    'cb_colsample_bylevel': 0.919245284773473,\n    'hgb_learning_rate': 0.2156436115943959,\n    'hgb_max_depth': 4,\n    'hgb_min_samples_leaf': 88,\n    'hgb_max_bins': 65\n}\n\n\n# LightGBM parameters\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state': random_state,\n    'lambda_l1': best_trial_params['lgb_lambda_l1'],\n    'lambda_l2': best_trial_params['lgb_lambda_l2'],\n    'num_leaves': best_trial_params['lgb_num_leaves'],\n    'feature_fraction': best_trial_params['lgb_feature_fraction'],\n    'bagging_fraction': best_trial_params['lgb_bagging_fraction'],\n    'bagging_freq': best_trial_params['lgb_bagging_freq'],\n    'min_child_samples': best_trial_params['lgb_min_child_samples'],\n    'learning_rate': best_trial_params['lgb_learning_rate']\n}\n\n# XGBoost parameters\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state': 42,\n    'lambda': best_trial_params['xgb_lambda'],\n    'alpha': best_trial_params['xgb_alpha'],\n    'max_depth': best_trial_params['xgb_max_depth'],\n    'eta': best_trial_params['xgb_eta'],\n    'subsample': best_trial_params['xgb_subsample'],\n    'colsample_bytree': best_trial_params['xgb_colsample_bytree']\n}\n\n# CatBoost parameters\ncb_params = {\n    'objective': 'MultiClass',\n    'verbose': 0,\n    'random_seed': random_state,\n    'iterations': best_trial_params['iterations'],\n    'learning_rate': best_trial_params['cb_learning_rate'],\n    'depth': best_trial_params['cb_depth'],\n    'l2_leaf_reg': best_trial_params['cb_l2_leaf_reg'],\n    'colsample_bylevel': best_trial_params['cb_colsample_bylevel']\n}\n\n# HistBoost parameters\nhgb_params = {\n    'max_iter': 100,\n    'random_state' : random_state,\n    'learning_rate': best_trial_params['hgb_learning_rate'],\n    'max_depth': best_trial_params['hgb_max_depth'],\n    'min_samples_leaf': best_trial_params['hgb_min_samples_leaf'],\n    'max_bins': best_trial_params['hgb_max_bins']\n}\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nm_test_accuracies = []\ns_test_accuracies = []\nh_test_accuracies = []\n\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Training LightGBM\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n    # Training XGBoost\n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n    \n    # Training CatBoost\n    cb_model = cb.CatBoostClassifier(**cb_params)\n    cb_model.fit(X_train, y_train)\n    \n    # Training HistGradBoost\n    hgb_model = HistGradientBoostingClassifier(**hgb_params)\n    hgb_model.fit(X_train, y_train)\n\n    lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n    xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n    cb_pred = cb_model.predict_proba(X_test)\n    hgb_pred = hgb_model.predict_proba(X_test)\n\n    # Mean \n    ensemble_pred = np.mean([lgb_pred, xgb_pred, cb_pred, hgb_pred], axis=0)\n    final_pred = np.argmax(ensemble_pred, axis=1)\n    test_accuracy = accuracy_score(y_test, final_pred)\n    m_test_accuracies.append(test_accuracy)\n    \nprint(f\"Mean ensemble accuracy: {np.mean(m_test_accuracies)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T12:40:19.158711Z","iopub.execute_input":"2024-02-07T12:40:19.159097Z","iopub.status.idle":"2024-02-07T12:43:18.147885Z","shell.execute_reply.started":"2024-02-07T12:40:19.159067Z","shell.execute_reply":"2024-02-07T12:43:18.14672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\n\ntest_ids = test_df['id'].copy()\ntest_df.drop(columns=['id'], inplace=True)\n\ntest_df['CALC'] = np.where(test_df['CALC'] == 'Always', 'Frequently', test_df['CALC'])\ntest_df[categorical_features] = ordinal_encoder.transform(test_df[categorical_features])\ntest_df[non_categorical_features.columns] = scaler.transform(test_df[non_categorical_features.columns])\n\nX_test = test_df  \nX_test_dmatrix = xgb.DMatrix(test_df) \n\nlgb_pred_test = lgb_model.predict(test_df, num_iteration=lgb_model.best_iteration)\nxgb_pred_test = xgb_model.predict(X_test_dmatrix)\ncb_pred_test = cb_model.predict_proba(X_test)\nhgb_pred_test = hgb_model.predict_proba(X_test)\n\n\nensemble_pred_test = np.mean([lgb_pred_test, xgb_pred_test, cb_pred_test, hgb_pred_test], axis=0)\nfinal_pred_test = np.argmax(ensemble_pred_test, axis=1)\ny_pred_submission_labels = label_encoder.inverse_transform(final_pred_test)\n\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'NObeyesdad': y_pred_submission_labels\n})\n\nsubmission_df.to_csv('ensemble_submission_pre01.csv', index=False)\n\nprint(\"Ensemble submission file created successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T12:24:40.376992Z","iopub.execute_input":"2024-02-07T12:24:40.377396Z","iopub.status.idle":"2024-02-07T12:24:41.991029Z","shell.execute_reply.started":"2024-02-07T12:24:40.377366Z","shell.execute_reply":"2024-02-07T12:24:41.98915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> Best Params on Original+Comp, [91.77%]  </span>\n","metadata":{}},{"cell_type":"code","source":"# best_trial_params = {\n#     'lgb_lambda_l1': 0.04809795705900344,\n#     'lgb_lambda_l2': 8.32132677405249e-05,\n#     'lgb_num_leaves': 124,\n#     'lgb_feature_fraction': 0.40306668077710556,\n#     'lgb_bagging_fraction': 0.5415059760725607,\n#     'lgb_bagging_freq': 5,\n#     'lgb_min_child_samples': 27,\n#     'lgb_learning_rate': 0.0648115973409096,\n#     'xgb_lambda': 0.022079309936168762,\n#     'xgb_alpha': 1.1861352308254374e-07,\n#     'xgb_max_depth': 10,\n#     'xgb_eta': 0.24898070594702448,\n#     'xgb_subsample': 0.5587544137677529,\n#     'xgb_colsample_bytree': 0.42772532158291765,\n#     'iterations': 337,\n#     'cb_l2_leaf_reg': 0.0022099509714579076,\n#     'cb_depth': 5,\n#     'cb_learning_rate': 0.08836676515397127,\n#     'cb_colsample_bylevel': 0.6368269699495822,\n#     'hgb_learning_rate': 0.16627306296115918,\n#     'hgb_max_depth': 3,\n#     'hgb_min_samples_leaf': 8,\n#     'hgb_max_bins': 147\n# }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\ncomp_df = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ncomp_df = comp_df.drop(columns=['id'])\n\ncombined_df = pd.concat([df, comp_df], axis=0)\ncombined_df = combined_df.drop_duplicates()\ncombined_df.reset_index(drop=True, inplace=True)\n\ncategorical_features = combined_df.select_dtypes(include=['object']).columns\ncategorical_features = [col for col in categorical_features if col != 'NObeyesdad']\nnon_categorical_features = combined_df.select_dtypes(exclude=['object'])\ncombined_df['CALC'] = np.where(combined_df['CALC'] == 'Always', 'Frequently', combined_df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ncombined_df[categorical_features] = ordinal_encoder.fit_transform(combined_df[categorical_features])\n\nscaler = StandardScaler()\ncombined_df[non_categorical_features.columns] = scaler.fit_transform(combined_df[non_categorical_features.columns])\n\nX = combined_df.drop(columns=['NObeyesdad'])\ny = combined_df['NObeyesdad']\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n\n\nbest_trial_params = {\n    'lgb_lambda_l1': 0.04809795705900344,\n    'lgb_lambda_l2': 8.32132677405249e-05,\n    'lgb_num_leaves': 124,\n    'lgb_feature_fraction': 0.40306668077710556,\n    'lgb_bagging_fraction': 0.5415059760725607,\n    'lgb_bagging_freq': 5,\n    'lgb_min_child_samples': 27,\n    'lgb_learning_rate': 0.0648115973409096,\n    'xgb_lambda': 0.022079309936168762,\n    'xgb_alpha': 1.1861352308254374e-07,\n    'xgb_max_depth': 10,\n    'xgb_eta': 0.24898070594702448,\n    'xgb_subsample': 0.5587544137677529,\n    'xgb_colsample_bytree': 0.42772532158291765,\n    'iterations': 337,\n    'cb_l2_leaf_reg': 0.0022099509714579076,\n    'cb_depth': 5,\n    'cb_learning_rate': 0.08836676515397127,\n    'cb_colsample_bylevel': 0.6368269699495822,\n    'hgb_learning_rate': 0.16627306296115918,\n    'hgb_max_depth': 3,\n    'hgb_min_samples_leaf': 8,\n    'hgb_max_bins': 147\n}\n\n# LightGBM parameters\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state': 42,\n    'lambda_l1': best_trial_params['lgb_lambda_l1'],\n    'lambda_l2': best_trial_params['lgb_lambda_l2'],\n    'num_leaves': best_trial_params['lgb_num_leaves'],\n    'feature_fraction': best_trial_params['lgb_feature_fraction'],\n    'bagging_fraction': best_trial_params['lgb_bagging_fraction'],\n    'bagging_freq': best_trial_params['lgb_bagging_freq'],\n    'min_child_samples': best_trial_params['lgb_min_child_samples'],\n    'learning_rate': best_trial_params['lgb_learning_rate']\n}\n\n# XGBoost parameters\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state': 42,\n    'lambda': best_trial_params['xgb_lambda'],\n    'alpha': best_trial_params['xgb_alpha'],\n    'max_depth': best_trial_params['xgb_max_depth'],\n    'eta': best_trial_params['xgb_eta'],\n    'subsample': best_trial_params['xgb_subsample'],\n    'colsample_bytree': best_trial_params['xgb_colsample_bytree']\n}\n\n# CatBoost parameters\ncb_params = {\n    'objective': 'MultiClass',\n    'verbose': 0,\n    'random_seed': 42,\n    'iterations': best_trial_params['iterations'],\n    'learning_rate': best_trial_params['cb_learning_rate'],\n    'depth': best_trial_params['cb_depth'],\n    'l2_leaf_reg': best_trial_params['cb_l2_leaf_reg'],\n    'colsample_bylevel': best_trial_params['cb_colsample_bylevel']\n}\n\n# HistBoost parameters\nhgb_params = {\n    'max_iter': 100,\n    'random_state' : 42,\n    'learning_rate': best_trial_params['hgb_learning_rate'],\n    'max_depth': best_trial_params['hgb_max_depth'],\n    'min_samples_leaf': best_trial_params['hgb_min_samples_leaf'],\n    'max_bins': best_trial_params['hgb_max_bins']\n}\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ntest_accuracies = []\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Training LightGBM\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)\n\n    # Training XGBoost\n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)\n    \n    # Training CatBoost\n    cb_model = cb.CatBoostClassifier(**cb_params)\n    cb_model.fit(X_train, y_train)\n    \n    # Training HistGradBoost\n    hgb_model = HistGradientBoostingClassifier(**hgb_params)\n    hgb_model.fit(X_train, y_train)\n\n    lgb_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n    xgb_pred = xgb_model.predict(xgb.DMatrix(X_test))\n    cb_pred = cb_model.predict_proba(X_test)\n    hgb_pred = hgb_model.predict_proba(X_test)\n\n    ensemble_pred = np.mean([lgb_pred, xgb_pred, cb_pred, hgb_pred], axis=0)\n    final_pred = np.argmax(ensemble_pred, axis=1)\n\n    test_accuracy = accuracy_score(y_test, final_pred)\n    test_accuracies.append(test_accuracy)\n\nprint(\"Average test set accuracy:\", np.mean(test_accuracies))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T15:17:03.099662Z","iopub.execute_input":"2024-02-07T15:17:03.100197Z","iopub.status.idle":"2024-02-07T15:19:40.104375Z","shell.execute_reply.started":"2024-02-07T15:17:03.100127Z","shell.execute_reply":"2024-02-07T15:19:40.103012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\n\ntest_ids = test_df['id'].copy()\ntest_df.drop(columns=['id'], inplace=True)\n\ntest_df['CALC'] = np.where(test_df['CALC'] == 'Always', 'Frequently', test_df['CALC'])\ntest_df[categorical_features] = ordinal_encoder.transform(test_df[categorical_features])\ntest_df[non_categorical_features.columns] = scaler.transform(test_df[non_categorical_features.columns])\n\nX_test = test_df  \nX_test_dmatrix = xgb.DMatrix(test_df) \n\nlgb_pred_test = lgb_model.predict(test_df, num_iteration=lgb_model.best_iteration)\nxgb_pred_test = xgb_model.predict(X_test_dmatrix)\ncb_pred_test = cb_model.predict_proba(X_test)\nhgb_pred_test = hgb_model.predict_proba(X_test)\n\n\nensemble_pred_test = np.mean([lgb_pred_test, xgb_pred_test, cb_pred_test, hgb_pred_test], axis=0)\nfinal_pred_test = np.argmax(ensemble_pred_test, axis=1)\ny_pred_submission_labels = label_encoder.inverse_transform(final_pred_test)\n\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'NObeyesdad': y_pred_submission_labels\n})\n\nsubmission_df.to_csv('ensemble_submission_pred02.csv', index=False)\n\nprint(\"Ensemble submission file created successfully.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:orange\"> Optuna Ensemble Original+Comp, 100% Train </span>\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv')\ncomp_df = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ncomp_df = comp_df.drop(columns=['id'])\n\ncombined_df = pd.concat([df, comp_df], axis=0)\ncombined_df = combined_df.drop_duplicates()\ncombined_df.reset_index(drop=True, inplace=True)\n\ncategorical_features = combined_df.select_dtypes(include=['object']).columns\ncategorical_features = [col for col in categorical_features if col != 'NObeyesdad']\nnon_categorical_features = combined_df.select_dtypes(exclude=['object'])\ncombined_df['CALC'] = np.where(combined_df['CALC'] == 'Always', 'Frequently', combined_df['CALC'])\n\nordinal_encoder = OrdinalEncoder()\ncombined_df[categorical_features] = ordinal_encoder.fit_transform(combined_df[categorical_features])\n\nscaler = StandardScaler()\ncombined_df[non_categorical_features.columns] = scaler.fit_transform(combined_df[non_categorical_features.columns])\n\nX = combined_df.drop(columns=['NObeyesdad'])\ny = combined_df['NObeyesdad']\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\nlgb_params = {\n    'objective': 'multiclass',\n    'num_class': 7,\n    'metric': 'multi_logloss',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'random_state' : 42,\n    'lambda_l1': 0.0013018662948233166,\n    'lambda_l2': 4.739507857074739e-06,\n    'num_leaves': 212,\n    'feature_fraction': 0.7326733564705078,\n    'bagging_fraction': 0.8171584187213554,\n    'bagging_freq': 6,\n    'min_child_samples': 24,\n    'learning_rate': 0.028803812378459442\n}\nxgb_params = {\n    'objective': 'multi:softprob',\n    'num_class': 7,\n    'verbosity': 0,\n    'random_state' : 42,\n    'lambda': 0.33966275620551073,\n    'alpha': 0.345850236628533,\n    'max_depth': 5,\n    'eta': 0.1752332016094693,\n    'subsample': 0.8692386920145995,\n    'colsample_bytree': 0.48796459030657685\n}\n\nlgb_train = lgb.Dataset(X, y)\nlgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)  \n\nxgb_train = xgb.DMatrix(X, label=y)\nxgb_model = xgb.train(xgb_params, xgb_train, num_boost_round=100)  \n\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\n\ntest_ids = test_df['id'].copy()\ntest_df.drop(columns=['id'], inplace=True)\n\ntest_df['CALC'] = np.where(test_df['CALC'] == 'Always', 'Frequently', test_df['CALC'])\ntest_df[categorical_features] = ordinal_encoder.transform(test_df[categorical_features])\ntest_df[non_categorical_features.columns] = scaler.transform(test_df[non_categorical_features.columns])\n\nX_test = test_df  \nX_test_dmatrix = xgb.DMatrix(test_df) \n\nlgb_pred_test = lgb_model.predict(test_df, num_iteration=lgb_model.best_iteration)\nxgb_pred_test = xgb_model.predict(X_test_dmatrix)\n\nensemble_pred_test = np.mean([lgb_pred_test, xgb_pred_test], axis=0)\nfinal_pred_test = np.argmax(ensemble_pred_test, axis=1)\ny_pred_submission_labels = label_encoder.inverse_transform(final_pred_test)\n\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'NObeyesdad': y_pred_submission_labels\n})\n\nsubmission_df.to_csv('ensemble_submission_100%.csv', index=False)\n\nprint(\"Ensemble submission file created successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-06T22:21:21.131524Z","iopub.execute_input":"2024-02-06T22:21:21.131979Z","iopub.status.idle":"2024-02-06T22:21:59.195938Z","shell.execute_reply.started":"2024-02-06T22:21:21.131922Z","shell.execute_reply":"2024-02-06T22:21:59.194813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}